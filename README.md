# Learning language models to understand idiomatic constructions using RAG-patterns
This is a repository for our research on new different ways to enhance LM's knowledge about idiomatic exrpessions.
We will present our main solution related to retriever augmented generation (RAG) systems. A detailed overview of this approach will be provided in the theoretical part. The main motivation for using and adapting this approach to our task is the already recognized effectiveness of RAG in teaching language models to specific tasks in the context of semantics. For example, if we need a model to answer a question and answer within a specific topic (for example, scientific research on genetically modified products), such a system with the inclusion of a language model will show much more accurate results than if we used only a transformer.
Also in this paper we explore various data science methods, not just NLP, in order to adapt them to the task of teaching non-positional constructions. Among the methods that we explored was Locality Sensitive Hashing (LSH), which is usually used to optimize similarity search for marketing purposes. At the same time, we found that the problem for classification specifically in our case may be the inability to find the right document due to the lack of direct uses of the target idiom in the corpus, so we sought to maximize collisions, thus finding synonyms. We also looked at various ways to improve learning by using the model's perplexity to detect and transform non-positional values. Finally, one of the developments was the application and adaptation of the machine learning method, also popular in marketing algorithms - Maximum Inner Product Search (MIPS).
